---
# Deploy GCS FUSE mount for streaming server
# File: playbooks/roles/streaming-server/tasks/deploy-gcs-mount.yml

- name: Get current region from bastion hostname (robust method)
  shell: "hostname | grep -o 'bastion-[a-z0-9-]*' | cut -d'-' -f2-"
  register: current_region

- name: Ensure app_name is set
  set_fact:
    app_name: "streaming-server"

- name: Ensure app_namespace is set
  set_fact:
    app_namespace: "streaming"

- name: Validate region extraction
  debug:
    msg: |
      ğŸ” Region extraction in GCS mount:
      - Hostname: {{ ansible_hostname }}
      - Extracted region: "{{ current_region.stdout }}"
      - Region length: {{ current_region.stdout | length }}

- name: Set bastion region fact
  set_fact:
    bastion_region: "{{ current_region.stdout }}"

- name: Validate region format
  debug:
    msg: |
      âœ… Region validation:
      - bastion_region: "{{ bastion_region }}"
      - Region is not empty: {{ bastion_region != "" }}

- name: Fail if region could not be extracted
  fail:
    msg: "Could not extract region from hostname: {{ ansible_hostname }}"
  when: bastion_region == "" or bastion_region == "unknown"

- name: Debug variable availability
  debug:
    msg: |
      ğŸ” Debug info:
      - bastion_region: {{ bastion_region }}
      - regional_bucket_names: {{ regional_bucket_names | default('NOT DEFINED') }}
      - regional_bucket_names type: {{ regional_bucket_names | type_debug if regional_bucket_names is defined else 'UNDEFINED' }}
      - content_reader_sa_email: {{ content_reader_sa_email | default('NOT DEFINED') }}

- name: Set regional bucket name with safer fallback
  set_fact:
    current_bucket_name: >-
      {%- if regional_bucket_names is defined and regional_bucket_names is mapping -%}
        {%- if bastion_region in regional_bucket_names -%}
          {{ regional_bucket_names[bastion_region] }}
        {%- else -%}
          {{ project_name }}-content-{{ bastion_region }}
        {%- endif -%}
      {%- else -%}
        {{ project_name }}-content-{{ bastion_region }}
      {%- endif -%}

- name: Display GCS mount deployment info
  debug:
    msg: |
      ğŸ—„ï¸  Deploying GCS FUSE mount for region: {{ bastion_region }}
      
      ğŸ“¦ Source: gs://{{ current_bucket_name }}
      ğŸ“ Mount path: /mnt/videos (read-only)
      ğŸ” Service account: {{ content_reader_sa_email | default('content-reader-sa@' + project_id + '.iam.gserviceaccount.com') }}
      
      ğŸ”„ Deployment phases:
      1ï¸âƒ£  Install GCS FUSE CSI driver
      2ï¸âƒ£  Create Workload Identity ServiceAccount  
      3ï¸âƒ£  Deploy storage components (StorageClass, PV, PVC)
      4ï¸âƒ£  Update DaemonSet to use GCS mount

- name: Phase 1 - Deploy GCS FUSE CSI driver
  include_tasks: deploy-csi-driver.yml
  tags: ['gcs-mount', 'csi-driver']

- name: Phase 2 - Deploy Workload Identity ServiceAccount
  include_tasks: deploy-workload-identity.yml
  tags: ['gcs-mount', 'workload-identity']

- name: Phase 3 - Deploy storage components
  include_tasks: deploy-storage.yml
  tags: ['gcs-mount', 'storage']

- name: Phase 4 - Verify all components are ready
  block:
    - name: Check CSI driver is running
      shell: |
        kubectl get pods -n gcs-fuse-csi-driver -l app=gcs-fuse-csi-driver --field-selector=status.phase=Running --no-headers | wc -l
      register: csi_driver_running

    - name: Check PVC is bound
      shell: |
        kubectl get pvc gcs-videos-pvc -n {{ app_namespace }} -o jsonpath='{.status.phase}'
      register: pvc_bound_status

    - name: Check ServiceAccount exists
      shell: |
        kubectl get sa {{ app_name }}-sa -n {{ app_namespace }} --no-headers | wc -l
      register: sa_exists

    - name: Verify all components are ready
      assert:
        that:
          - csi_driver_running.stdout | int > 0
          - pvc_bound_status.stdout == "Bound"
          - sa_exists.stdout | int > 0
        fail_msg: |
          ğŸš¨ GCS mount components not ready:
          - CSI driver pods running: {{ csi_driver_running.stdout }}
          - PVC status: {{ pvc_bound_status.stdout }}
          - ServiceAccount exists: {{ sa_exists.stdout }}
        success_msg: "âœ… All GCS mount components are ready"

- name: Display GCS mount deployment summary
  debug:
    msg: |
      âœ… GCS FUSE Mount Deployment Complete for {{ bastion_region }}
      
      ğŸ“Š Summary:
      ğŸ”§ CSI driver: {{ csi_driver_running.stdout }} pods running
      ğŸ” ServiceAccount: {{ app_name }}-sa (Workload Identity enabled)
      ğŸ’¾ StorageClass: gcs-fuse-{{ bastion_region }}
      ğŸ“¦ PersistentVolume: gcs-videos-pv-{{ bastion_region }}
      ğŸ¯ PersistentVolumeClaim: gcs-videos-pvc ({{ pvc_bound_status.stdout }})
      
      ğŸ—‚ï¸  Content source: gs://{{ current_bucket_name }}
      ğŸ“ Mount path in pods: /mnt/videos (read-only)
      
      ğŸ¯ Next step: Update DaemonSet to use the GCS mount
      
      âš ï¸  Important notes:
      - Pods must use serviceAccountName: {{ app_name }}-sa
      - Content synced from master bucket appears in /mnt/videos
      - First mount may take 30-60 seconds for FUSE initialization
      - Content is cached locally for better performance

- name: Create test content verification script
  template:
    src: test-gcs-mount.sh.j2
    dest: "{{ manifests_dir }}/test-gcs-mount.sh"
    mode: '0755'

- name: Display post-deployment instructions
  debug:
    msg: |
      ğŸš€ GCS Mount Ready! Next Steps:
      
      1ï¸âƒ£  Update your DaemonSet to use:
         serviceAccountName: {{ app_name }}-sa
      
      2ï¸âƒ£  Verify mount works:
         kubectl exec -it <pod-name> -n {{ app_namespace }} -- ls -la /mnt/videos/
      
      3ï¸âƒ£  Test content access:
         {{ manifests_dir }}/test-gcs-mount.sh
      
      4ï¸âƒ£  Sync content to bucket:
         # From admin VM:
         /opt/content/scripts/sync-all-content.sh
      
      5ï¸âƒ£  Access content via web:
         https://{{ domain_name }}/videos/your-video.mp4